{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e9c809c-c066-4d36-9c56-7b04f4e387b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import sys\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.components import OutputPath, InputPath\n",
    "from kfp.v2.dsl import Input, Output, Dataset, Model\n",
    "import json\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edc1c14c-15ea-4551-b7b5-318d2d78c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=\"INFO\", stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6866f57a-8966-4840-ab87-1895f2e51ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Getting dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "INFO:root:Spliting dataset\n",
      "INFO:root:Fittig model with train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_161127/2469419203.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.rename(columns = {\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "INFO:root:Computing scores\n",
      "INFO:root:Model AUC Score: 1.0\n",
      "INFO:root:Accuracy test score: 0.9666666666666667\n",
      "INFO:root:Saving model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.basicConfig(level=\"INFO\", stream=sys.stdout)\n",
    "\n",
    "model_path = \"./model.pkl\"\n",
    "def save_model_pickle(model, filename):\n",
    "    \"\"\"\n",
    "    Saves a ML model in pickle format\n",
    "    Args: model - The model bytes object to be saved\n",
    "          filename - The filename\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as pickle_file:\n",
    "        pickle.dump(model, pickle_file)\n",
    "\n",
    "\n",
    "def get_iris_data():\n",
    "    \"\"\"\n",
    "    Get iris dataset from UCI reposiory\n",
    "    Returns: \n",
    "        X  - Dataframe containing 4 features regrding iris characteristics\n",
    "        y - The target array for the iris classification \n",
    "    \"\"\"\n",
    "    data_iris = fetch_ucirepo(id=53) \n",
    "    X = data_iris.data.features \n",
    "    X.rename(columns = {\n",
    "        'sepal length' : 'sepal_length',\n",
    "        'sepal width' : 'sepal_width',\n",
    "        'petal length' : 'petal_length',\n",
    "        'petal width': 'petal_width'\n",
    "    }, inplace=True)\n",
    "    print(type(X))\n",
    "    y = data_iris.data.targets['class']\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_training_pipeline():\n",
    "    \"Defines training pipeline steps and returns the pipeline\"\n",
    "    pipeline = Pipeline(steps = [\n",
    "        ('Imputer', SimpleImputer(strategy='mean', keep_empty_features=True)),\n",
    "        ('normalization', StandardScaler()),\n",
    "        ('estimator', LogisticRegression() )\n",
    "    ]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "def fit_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fit a ML pipeline using GridSearchCV with 5 folds\n",
    "    Args: X_train - Dataset to be trained\n",
    "          y_train - Target column \n",
    "    Returns: model - ML pipeline fitted\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "        'estimator__solver': ['newton-cg'],\n",
    "        'estimator__tol': [ 0.0001, 0.003, 0.01],\n",
    "        'estimator__penalty': [None, 'l2'],\n",
    "    }\n",
    "\n",
    "    pipeline = get_training_pipeline()\n",
    "    model = GridSearchCV(estimator=pipeline,\n",
    "                            param_grid=parameters,\n",
    "                            scoring= {\"AUC\": \"roc_auc_ovr\"},\n",
    "                            refit=\"AUC\",\n",
    "                            cv=5,\n",
    "                            verbose=1,\n",
    "                            error_score='raise')\n",
    "    model = model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_training_pipeline():\n",
    "    \"\"\"\n",
    "        Runs the trainin pipelie for building a classifier for iris dataset and\n",
    "        saves the trained model\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(f\"Getting dataset\")\n",
    "    X, y = get_iris_data()\n",
    "\n",
    "    logging.info(f\"Spliting dataset\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=14)\n",
    "\n",
    "    logging.info(f\"Fittig model with train data\")\n",
    "    cv_model = fit_model(X_train, y_train)\n",
    "    y_pred = cv_model.predict(X_test)\n",
    "\n",
    "    logging.info(f\"Computing scores\")\n",
    "    model_score = cv_model.score(X_test, y_test)\n",
    "    logging.info(f\"Model AUC Score: {model_score}\")\n",
    "\n",
    "    test_acc_score = accuracy_score(y_test, y_pred)\n",
    "    logging.info(f\"Accuracy test score: {test_acc_score}\")\n",
    "\n",
    "    logging.info(\"Saving model\")\n",
    "    save_model_pickle(cv_model, model_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_training_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d83c23-9643-45c1-a1ba-453101b73bc9",
   "metadata": {},
   "source": [
    "Creating the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0bdba63-3d8b-4dca-bced-ac7d9d684f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=[\"ucimlrepo==0.0.7\",\"fastparquet==2023.7.0\"], base_image=\"python:3.9\")\n",
    "def load_data()-> Dataset:\n",
    "    \"\"\"\n",
    "    Get iris dataset from UCI reposiory\n",
    "    Returns: \n",
    "        X  - Dataframe containing 4 features regrding iris characteristics\n",
    "        y - The target array for the iris classification \n",
    "    \"\"\"\n",
    "    import logging\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "    logging.info(\"Getting Dataset\")\n",
    "    data_iris = fetch_ucirepo(id=53) \n",
    "    X_array = data_iris.data.features \n",
    "    X_array.rename(columns = {\n",
    "        'sepal length' : 'sepal_length',\n",
    "        'sepal width' : 'sepal_width',\n",
    "        'petal length' : 'petal_length',\n",
    "        'petal width': 'petal_width'\n",
    "    }, inplace=True)\n",
    "    y_array = data_iris.data.targets['class']\n",
    "\n",
    "    X_array['target'] = y_array\n",
    "\n",
    "    data_artifact = Dataset(name=\"dataset\", metadata={})\n",
    "    X_array.to_csv(data_artifact.path)\n",
    "\n",
    "    return data_artifact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "26669387-76ff-420e-a7f4-d25928b4b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=[\"scikit-learn==1.5.2\"], base_image=\"python:3.9\")\n",
    "def set_training_pipeline(pipeline_out: Output[Model])->Model:\n",
    "    \"\"\"\n",
    "    Defines training pipeline steps and returns the pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    import logging\n",
    "    import joblib\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    pipeline = Pipeline(steps = [\n",
    "        ('Imputer', SimpleImputer(strategy='mean', keep_empty_features=True)),\n",
    "        ('normalization', StandardScaler()),\n",
    "        ('estimator', LogisticRegression() )\n",
    "        ]\n",
    "    )\n",
    "    # model_artifact = Model(name=\"model\", metadata={}, uri=dsl.get_uri())\n",
    "    joblib.dump(pipeline, pipeline_out.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2e44ef6-70af-44bc-8990-dda9578665de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=[\"scikit-learn==1.5.2\"], base_image=\"python:3.9\")\n",
    "def train_model(\n",
    "    dataset: Input[Dataset],\n",
    "    pipeline: Input[Model],\n",
    ")->Model:\n",
    "    import logging\n",
    "    import joblib\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "    logging.info(f\"Spliting dataset\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.drop(columns=\"target\"), dataset[\"target\"], stratify=dataset[\"target\"], test_size=0.2, random_state=14)\n",
    "\n",
    "    logging.info(f\"Fittig model with train data\")\n",
    "\n",
    "\n",
    "    parameters = {\n",
    "        'estimator__solver': ['newton-cg'],\n",
    "        'estimator__tol': [ 0.0001, 0.003, 0.01],\n",
    "        'estimator__penalty': [None, 'l2'],\n",
    "    }\n",
    "\n",
    "    model = GridSearchCV(estimator=pipeline,\n",
    "                            param_grid=parameters,\n",
    "                            scoring= {\"AUC\": \"roc_auc_ovr\"},\n",
    "                            refit=\"AUC\",\n",
    "                            cv=5,\n",
    "                            verbose=1,\n",
    "                            error_score='raise')\n",
    "    \n",
    "    pipeline = joblib.load(pipeline)\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    logging.info(f\"Computing scores\")\n",
    "    model_score = model.score(X_test, y_test)\n",
    "    logging.info(f\"Model AUC Score: {model_score}\")\n",
    "\n",
    "    test_acc_score = accuracy_score(y_test, y_pred)\n",
    "    logging.info(f\"Accuracy test score: {test_acc_score}\")\n",
    "\n",
    "    logging.info(\"Saving model\")\n",
    "    model_artifact = Model(name=\"model\", metadata={})\n",
    "    joblib.dump(model, model_artifact.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ddc346c0-c42d-4fd2-a224-52a0a6c8eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT=\"\"\n",
    "@dsl.pipeline(\n",
    "    name=\"my-pipeline-\",\n",
    "    description=\"A class project\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def my_pipeline_func():\n",
    "    load_data_component = load_data()\n",
    "    set_training_pipe_component = set_training_pipeline(\n",
    "        \n",
    "    ).after(load_data_component)\n",
    "\n",
    "    fit_model_component = train_model(\n",
    "        dataset = load_data_component.output,\n",
    "        pipeline = set_training_pipe_component.outputs['pipeline_out']\n",
    "    ).after(set_training_pipe_component)\n",
    "\n",
    "\n",
    "    # log_model_component = log_model(\n",
    "    #     fit_model_component.outputs.\n",
    "        \n",
    "    # ).after(fit_model_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d3d26c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_pipeline(pipeline_func):   \n",
    "    compiler.Compiler().compile(\n",
    "        pipeline_func=pipeline_func,\n",
    "        package_path=\"./temp/my_pipeline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ffb60ab4-9c59-473f-9ff1-1af6bbf22928",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not PipelineParam",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompile_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_pipeline_func\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[119], line 2\u001b[0m, in \u001b[0;36mcompile_pipeline\u001b[0;34m(pipeline_func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompile_pipeline\u001b[39m(pipeline_func):   \n\u001b[0;32m----> 2\u001b[0m     \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpackage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./temp/my_pipeline.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aula-project/venv/lib/python3.12/site-packages/kfp/v2/compiler/compiler.py:1176\u001b[0m, in \u001b[0;36mCompiler.compile\u001b[0;34m(self, pipeline_func, package_path, pipeline_name, pipeline_parameters, type_check)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     kfp\u001b[38;5;241m.\u001b[39mTYPE_CHECK \u001b[38;5;241m=\u001b[39m type_check\n\u001b[1;32m   1175\u001b[0m     kfp\u001b[38;5;241m.\u001b[39mCOMPILING_FOR_V2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1176\u001b[0m     pipeline_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_pipeline_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_parameters_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_pipeline(pipeline_job, package_path)\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/aula-project/venv/lib/python3.12/site-packages/kfp/v2/compiler/compiler.py:1107\u001b[0m, in \u001b[0;36mCompiler._create_pipeline_v2\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_parameters_override)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     args_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1103\u001b[0m         dsl\u001b[38;5;241m.\u001b[39mPipelineParam(\n\u001b[1;32m   1104\u001b[0m             sanitize_k8s_name(arg_name, \u001b[38;5;28;01mTrue\u001b[39;00m), param_type\u001b[38;5;241m=\u001b[39marg_type))\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dsl\u001b[38;5;241m.\u001b[39mPipeline(pipeline_name) \u001b[38;5;28;01mas\u001b[39;00m dsl_pipeline:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[43mpipeline_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_exit_handler(dsl_pipeline)\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_and_inject_artifact(dsl_pipeline)\n",
      "Cell \u001b[0;32mIn[118], line 15\u001b[0m, in \u001b[0;36mmy_pipeline_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m load_data_component \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m      9\u001b[0m set_training_pipe_component \u001b[38;5;241m=\u001b[39m set_training_pipeline(\n\u001b[1;32m     10\u001b[0m     \n\u001b[1;32m     11\u001b[0m )\u001b[38;5;241m.\u001b[39mafter(load_data_component)\n\u001b[1;32m     13\u001b[0m fit_model_component \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     14\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m load_data_component\u001b[38;5;241m.\u001b[39moutput,\n\u001b[0;32m---> 15\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mset_training_pipe_component\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpipeline_out\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m )\u001b[38;5;241m.\u001b[39mafter(set_training_pipe_component)\n",
      "File \u001b[0;32m~/aula-project/venv/lib/python3.12/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not PipelineParam"
     ]
    }
   ],
   "source": [
    "compile_pipeline(my_pipeline_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cae67e9b-cb32-4ed8-b813-505f75f07ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/367188317574/locations/us-central1/pipelineJobs/my-pipeline-20250124141646\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/367188317574/locations/us-central1/pipelineJobs/my-pipeline-20250124141646')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/my-pipeline-20250124141646?project=367188317574\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/367188317574/locations/us-central1/pipelineJobs/my-pipeline-20250124141646 current state:\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m aiplatform\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersonal-448814\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m                 location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-central1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                 staging_bucket\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://kfp_bucket_pipeline/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPIPELINE_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m job \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mPipelineJob(\n\u001b[1;32m      8\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA pipeline for a class project\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     template_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./temp/my_pipeline.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-central1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpersonal@personal-448814.iam.gserviceaccount.com\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aula-project/venv/lib/python3.12/site-packages/google/cloud/aiplatform/base.py:669\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait()\n\u001b[0;32m--> 669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    672\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/aula-project/venv/lib/python3.12/site-packages/google/cloud/aiplatform/pipeline_jobs.py:255\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[0;34m(self, service_account, network, sync)\u001b[0m\n\u001b[1;32m    249\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_create_complete_with_getter(\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline_job\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m )\n\u001b[1;32m    253\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mView Pipeline Job:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dashboard_uri())\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aula-project/venv/lib/python3.12/site-packages/google/cloud/aiplatform/pipeline_jobs.py:306\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m         log_wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(log_wait \u001b[38;5;241m*\u001b[39m multiplier, max_wait)\n\u001b[1;32m    305\u001b[0m         previous_time \u001b[38;5;241m=\u001b[39m current_time\n\u001b[0;32m--> 306\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Running pipeline\n",
    "\n",
    "PIPELINE_ROOT = \"temp\"\n",
    "aiplatform.init(project=\"personal-448814\",\n",
    "                location=\"us-central1\",\n",
    "                staging_bucket=f\"gs://kfp_bucket_pipeline/{PIPELINE_ROOT}/\")\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"A pipeline for a class project\",\n",
    "    template_path=\"./temp/my_pipeline.json\",\n",
    "    pipeline_root=f\"gs://kfp_bucket_pipeline/{PIPELINE_ROOT}/\",\n",
    "    project=\"personal-448814\",\n",
    "    location=\"us-central1\",\n",
    ")\n",
    "job.run(service_account=\"personal@personal-448814.iam.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e74c71d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://kfp_bucket_pipeline/temp/'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"gs://kfp_bucket_pipeline/{PIPELINE_ROOT}/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
